# 1.4 设置文件

## 参数

### 设置UA

`USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'`

### robots.txt

是否遵循robots爬虫规则

`ROBOTSTXT_OBEY = False`

- False
- True

### 日志等级

`LOG_LEVEL = 'INFO'`

- `DEBUG`: 调试级别，输出所有日志
- `INFO`: 信息级别，输出普通信息
- `WARNING`: 警告级别，输出警告信息
- `ERROR`: 错误级别，输出错误信息
- `CRITICAL`: 严重错误级别，输出严重错误信息
- `FATAL`: 致命错误级别，输出致命错误信息
- `DISABLED`: 禁用级别，不输出任何日志
- `ALL`: 所有级别，输出所有日志
- `NOTSET`: 不设置级别，输出所有日志

### 并发数

默认是16

CONCURRENT_REQUESTS

### 管道

[相关](./03.0%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8.md)

```python
        ITEM_PIPELINES = {
            'parsepro.pipelines.ParseproPipeline': 300,
        }
```

### 去重

你已经不用去配置了，他内部的设置已经定义过了，如果你想自己写，就去把他的覆盖了

```python
DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
```

<CommentService/>
